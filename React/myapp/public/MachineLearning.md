#### PCA

输入：数据集 ![[公式]](https://www.zhihu.com/equation?tex=X%3D%5Cleft%5C%7B+x_%7B1%7D%2Cx_%7B2%7D%2Cx_%7B3%7D%2C...%2Cx_%7Bn%7D+%5Cright%5C%7D) ，需要降到k维。

1) 去平均值(即去中心化)，即每一位特征减去各自的平均值。

2) 计算协方差矩阵 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7DXX%5ET),注：这里除或不除样本数量n或n-1,其实对求出的特征向量没有影响。

3) 用特征值分解方法求协方差矩阵![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7DXX%5ET) 的特征值与特征向量。

4) 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。

5) 将数据转换到k个特征向量构建的新空间中，即Y=PX。



#### **DBSCAN理论--基本步骤**

**输入：**包含n个对象的集合D，指定半径epislon和最少样本量MinPts。

**输出：**所有生成的簇，达到密度要求。

1）repeat

2）从集合D中抽取一个未处理的点；

3）如果抽出的点是核心点，则找出所有从该点出发的密度可达对象，形成簇；

4）如果抽出点的为非核心点，则跳出循环，寻找下一个点；

5）until所有点都被处理。



#### EM

**算法步骤：**

（1）随机初始化模型参数θ的初值 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7B0%7D) 。

（2）j=1,2,...,J 开始EM算法迭代：

- E步：计算联合分布的条件概率期望：

![[公式]](https://www.zhihu.com/equation?tex=Q_%7Bi%7D%28z_%7Bi%7D%29%3Dp%28z_%7Bi%7D%7Cx_%7Bi%7D%2C%5Ctheta_%7Bj%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=l%28%5Ctheta%2C%5Ctheta_%7Bj%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Csum_%7Bz_%7Bi%7D%7D%5E%7B%7D%7BQ_%7Bi%7D%28z_%7Bi%7D%29log%5Cfrac%7Bp%28x_%7Bi%7D%2Cz_%7Bi%7D%3B%5Ctheta%29%7D%7BQ_%7Bi%7D%28z_%7Bi%7D%29%7D%7D%7D)

- M步：极大化 ![[公式]](https://www.zhihu.com/equation?tex=l%28%5Ctheta%2C%5Ctheta_%7Bj%7D%29) ,得到 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj%2B1%7D) :

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj%2B1%7D%3Dargmaxl%28%5Ctheta%2C%5Ctheta_%7Bj%7D%29)a

- 如果![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj%2B1%7D) 已经收敛，则算法结束。否则继续进行E步和M步进行迭代。

**输出：**模型参数θ。



#### SVM

![[公式]](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2+)

![[公式]](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cge+1%2C%5C+i%3D1%2C2%2C...%2CN+)







Q1。深度学习和机器学习的区别是什么?

机器学习涉及从数据模式中学习，然后将其应用于决策的算法。深度学习能够通过自身处理数据进行学习，它与人类大脑非常相似，能够识别、分析并做出决定。

主要区别如下:数据呈现给系统的方式。机器学习算法总是需要结构化的数据，而深度学习网络依赖于人工神经网络的层次。



Q2。监督和非监督机器学习的主要区别是什么?

监督学习与非监督学习的主要区别在于:监督学习技术需要有标记的数据来训练模型。例如，要解决一个分类问题(一个有监督的学习任务)，您需要有标签数据来训练模型，并将数据分类到您的标签组中。无监督学习不需要任何标记数据集(聚类算法）。



Q3。在处理数据集时如何选择重要的特征?

从数据集中选择重要特征的方法有以下几种:

1. 丢弃相关特征。
2. lasso回归。
3. 随机森林。
4. 基于信息增益的可用特征集。



Q4。目前有很多机器学习算法,如果给定一个数据集，如何确定使用哪个算法?

要使用的机器学习算法完全取决于给定数据集中的数据类型。如果数据是线性的，那么我们使用线性回归。如果数据显示非线性，那么bagging/kernel算法会做得更好。如果数据是为了一些商业目的而进行分析/解释，那么我们可以使用决策树或SVM。如果数据集由图像、视频、音频组成，神经网络将有助于精确求解。



Q5。什么时候正则化开始在机器学习中发挥作用?

当模型开始过拟合时，正则化就变得必要了。它的单词本意应该是规则化，添加一些规则。它的作用是在于限制模型中参数，让模型的参数不会太大，使得模型复杂度降低。无论是在线性模型还是神经网络中都可以通过这样简单的方式对模型中的参数进行限制，从而**减少**模型的**过拟合**的可能，提高预测能力。



Q6。数据的高方差是好还是坏?

较高的方差直接意味着数据分布较散乱，特征取值范围大。通常，具有高方差代表数据质量不好。



Q7。如果您的数据集存在高方差，您将如何处理它?

对于高方差的数据集，我们可以使用bagging算法来处理它。Bagging算法将数据分割成子组，从随机数据中复制抽样。数据分割后，使用随机数据使用训练算法创建规则。然后利用投票对模型的所有预测结果进行组合。



Q8。随机梯度下降(SGD)和梯度下降(GD)有什么区别?

梯度下降和随机梯度下降算法都能找出使损失函数最小化的一组参数。不同的是，在梯度下降，所有的训练样本评估每一组参数。而在随机梯度下降中，只对识别出的参数集评估一个训练样本



Q9。你能说说决策树的优点和缺点吗?

决策树的优点是它们更容易解释，是非参数的，因此对异常值具有鲁棒性，并且有相对较少的参数需要调整。

另一方面，缺点是它们容易过拟合，忽略属性间的相关性。





Q10。什么是混淆矩阵?你为什么需要它?



混淆矩阵是一个经常用来说明分类模型性能的表。它允许我们可视化算法/模型的性能。它被用作模型/算法的性能度量。用计数值总结正确和错误预测的数量，并按类别分类。它为我们提供了关于通过分类器所犯错误的信息以及分类器所犯错误的类型。

![image-20210704230208989](../AppData/Roaming/Typora/typora-user-images/image-20210704230208989.png)



Q11。解释一下“维度诅咒”这个短语。

维数的诅咒指的是数据有太多特征的情况。它涉及的问题如下:如果我们的特征比观察到的多，我们就有过拟合模型的风险。太多的维度会导致数据集中的每个观测结果与其他所有观测结果之间的距离相等（稀疏），没有任何有意义的聚类可以形成。在这种情况下，像PCA这样的降维技术可以帮助解决问题。



Q12。正则化和归一化有什么区别?

归一化校正数据;正则化调整预测函数。

如果您的数据处于非常不同的范围(特别是从低到高)，您可能希望规范化数据，统一量纲，缩放值的范围，这有助于确保不丢失准确性，加速模型收敛。

模型训练的目标之一是识别信号而忽略噪声，如果为了使误差最小化而对模型进行任意设置，就有可能出现过拟合。正则化通过提供更简单的拟合函数来控制这一点。



Q13。解释标准化和归一化之间的区别。

标准化和归一化是两种非常常用的方法。归一化是指重新缩放值以适应[0,1]的范围。标准化是指重新调整数据，使其均值为0，标准差为1。



Q14。区分回归和分类。

回归和分类被归类在监督机器学习的同一类别下。它们之间的主要区别是回归输出变量是数值的(或连续的)，而分类输出变量是分类的(或离散的)。例如:预测一个地方的确定温度是回归问题，而预测当天是晴天还是下雨是分类问题。



Q15。哪种机器学习算法被称为懒惰学习者?为什么它被称为懒惰学习者?

KNN是一种被称为懒学习者的机器学习算法。KNN是一种懒惰的学习者，因为它不从训练数据中学习任何机器学习的值或变量，而是在每次要分类时动态计算距离。



Q16。K-Means和KNN算法的区别?

KNN是监督学习，而K-Means是无监督学习。利用KNN，我们可以根据其最近的邻居预测未知元素的标签，并进一步扩展该方法以解决基于分类/回归的问题。K-Means是无监督学习，在这里我们没有任何标签，换句话说，没有目标变量，因此我们试图基于它们的坐标聚类数据，并试图建立基于为该聚类过滤的元素的聚类的性质





Q17。支持向量机算法的核技巧是什么?

核方法是一类把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。核函数输入两个向量，它返回的值跟两个向量分别作 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi) 映射然后点积的结果相同。核技巧是一种利用核函数直接计算 ![[公式]](https://www.zhihu.com/equation?tex=%5Clangle+%5Cphi%28x%29%2C%5Cphi%28z%29+%5Crangle) ，以避开分别计算 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28z%29) ，从而加速核方法计算的技巧。核方法不仅仅适用于SVM，还适用于其他数据为非线性可分的问题和算法。SVM的表现形式包含了映射的点积，所以可以用核技巧加速核方法的计算。





Q18。什么是集成模型?与传统的ML分类算法相比，集成技术如何产生更好的学习?

有时我们只能得到多个有偏好的模型（弱监督模型）。集成学习就是组合多个弱监督模型以期得到一个更好更全面的强监督模型。

与单一模型相比，它可以提供更好的预测性能，因为它结合了多个模型，减少了方差，平均了偏差，而且更不容易过拟合。



Q19。什么是过拟合和欠拟合?为什么决策树算法经常遇到过拟合问题?

过拟合指模型在训练集上表现很好，但在测试集上却表现很差。模型对训练集死记硬背，连噪声也记住了，没有理解数据背后的规律，泛化能力差。

欠拟合是一种当模型或算法显示出低方差但高偏差时，训练集上数据拟合不够好的现象。

在决策树中，当树被设计为完美地拟合训练数据集中的所有样本时，就会发生过拟合，这将导致具有严格规则或稀疏数据的分支，并影响训练集之外的样本的预测准确性。



Q20。请解释Lasso和岭回归Ridge之间的区别。

Lasso(L1)和Ridge(L2)是正则化技术，我们通过惩罚系数来找到最优解。Ridge的惩罚函数是由系数的平方和（L2范数）定义的。Lasso的惩罚系数是由绝对值之和（L1范数）定义的。另一种类型的正则化方法是ElasticNet弹性网络，它是一个混合惩罚函数，同时具有Lasso和ridge的优点。



Q21。概率和似然的区别是什么?

- “概率”描述了给定模型参数后，描述结果的合理性，而不涉及任何观察到的数据。

> 抛一枚均匀的硬币，拋20次，问15次拋得正面的可能性有多大？ 这里的可能性就是”概率”，均匀的硬币就是给定参数 $\theta=0.5$ ，“拋20次15次正面”是观测值$O$。求概率$P (H=15 | \theta=0.5) = ？$的概率。

- “似然”描述了给定了特定观测值后，描述模型参数是否合理。

> 拋一枚硬币，拋20次，结果15次正面向上，问其为均匀的可能性？ 这里的可能性就是”似然”，“拋20次15次正面”为观测值$O$为已知，参数$\theta=?$并不知道，求$L(\theta | H=15) = P (H=15 | \theta=0.5)$的最大化下的$\theta$ 值。





Q22。模型准确性还是模型性能?你更喜欢哪一个?

我们应该先弄清楚什么是模型性能?如果性能意味着速度，那么在实时应用场景中性能更重要。如果性能意味着计算开销小，那么在低性能终端上性能更重要。

如果应用场景对于结果的容错率低，那模型预测准确性更重要



Q23。如何处理不平衡的数据集?

采样技术可以帮助处理不平衡的数据集。有两种方法执行抽样，下抽样或过抽样。在Under Sampling中，我们减少了大多数类的大小来匹配少数类，因此有助于提高存储和运行时执行的性能，但它可能会丢弃有用的信息。对于过采样，我们对少量类进行上采样，从而解决了信息丢失的问题，但却遇到了过拟合的问题。



Q24。说说为什么特征工程在模型构建中很重要。

数据最初是原始形式的。在将数据提供给算法之前，需要从数据中提取特征。这个过程被称为特征工程。当你有相关的特征时，算法的复杂度就会降低。． 然后，即使使用了非理想的算法，结果也是准确的。

特性工程主要有两个目标：

1. 准备合适的输入数据集，以兼容机器学习算法的约束。
2. 提高机器学习模型的性能。





Q25。区别boosting和bagging?

boosting和bagging推是集成学习的变种。Bagging是一种用于减少方差的算法。Boosting是使用一个n-弱分类器系统进行预测，使每个弱分类器都能弥补其分类器的弱点的过程，有助于减少偏差。



Q26。生成性模型和区别性模型的区别是什么?生成模型学习联合概率分布p(x,y)。利用贝叶斯定理对条件概率进行预测。判别模型学习条件概率分布p(ylx)。这两种模型通常用于监督学习问题。一般来说，一个判别模型模型的决策边界之间的类。生成模型明确地模拟了每个类的实际分布。



Q27。什么是超参数，它们与参数有什么不同?

参数是模型内部的一个变量，它的值从训练数据中估计出来。它们通常被保存为学习模型的一部分。例如权重等超参数是模型外部的一个变量，它的值不能从数据中估计（手动调参）。它们经常被用来估计模型参数。超参数的选择对实现是敏感的。例子包括学习率，隐藏层层数等。





Q28。如何在聚类算法中定义聚类的数量?

集群的数量可以通过寻找轮廓系数（聚类效果好坏的一种评价方式）来确定。通常，我们的目标是使用聚类技术从数据中得到一些推论，这样我们就可以对数据所表示的许多类有一个更广泛的了解。在这种情况下，轮廓系数帮助我们确定聚类中心的数量，以便沿着这些聚类我们的数据。最简单有效的方法就是穷举1到N个聚类个数的情况，然后分别算似然。选似然最大的，可以保证找到似然上最优的。





Q29。交叉验证的作用是什么?

交叉验证是一种用于提高机器学习算法性能的技术，在这种技术中，机器将从相同的数据中多次输入采样数据。采样完成后，数据集被分成等量大小的部分，随机部分被选为测试集，而所有其他部分被选为训练集。

1. 交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。
2. 还可以从有限的数据中获取尽可能多的有效信息。



Q30。KNN中可以使用什么距离度量?

在KNN中可以使用欧氏距离，曼哈顿距离，闵可夫斯基距离。



Q31。随机森林使用了什么集成学习技术?

bagging《Random Forests》所使用的技术。随机森林是决策树的集合，它对原始数据集的采样数据进行工作，最终的预测是所有决策树结果的投票平均值。



Q32。熵和信息增益的区别是什么?

信息增益是基于对数据集进行属性分割后熵的减小。构建决策树的关键在于找到返回最高信息增益的属性(即，最同构的分支)。



Q33。给出一些你将使用SVM而不是随机森林的情况。

改用支持向量机的主要原因是问题可能不是线性可分的。在这种情况下，我们将不得不使用具有非线性核的支持向量机(例如RBF)。．

SVM模型在稀疏数据上的性能要优于树。例如，在文档分类中，您可能具有数千个甚至数万个特征，并且在任何给定的文档向量中，这些特征中只有一小部分的值可能大于零。



Q34。支持向量机作为一个大边际分类器，它是否受到极端值的影响?

说明:是的，如果C是大的，否则不是。



Q35。我们能把核技巧应用到逻辑回归中吗?

逻辑回归在计算上比支持向量机-O(N)比O(N2k)更昂贵，其中k是支持向量的数量。．SVM中的分类器被设计成仅根据支持向量定义，而在Logistic回归中，分类器是在所有点上定义的，而不仅仅是支持向量。这使得支持向量机可以享受一些自然的加速(在高效代码编写方面)，而这是逻辑回归难以实现的



Q36。支持向量机是否给出任何概率输出?

支持向量机不直接提供概率估计，这些是通过昂贵的交叉验证来计算的



Q37。说明贝叶斯风险和经验风险之间的区别.

贝叶斯决策函数f:X-cA是在所有可能函数中实现最小风险的函数。一个贝叶斯决策函数的风险称为贝叶斯风险。贝叶斯决策函数通常被称为“目标函数”，因为它是我们可能产生的最佳决策函数。f:X-cA相对于Dn的经验风险为* ()e (f (x, y)。



Q38。什么是拉格朗日乘子法?

在最优化问题中，拉格朗日乘子法是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法。这种方法可以将一个有*n*个变量与*k*个约束条件的最优化问题转换为一个解有*n* + *k*个变量的[方程](https://zh.wikipedia.org/wiki/方程)组的解的问题，拉格朗日对偶函数给出了最优解的下界。



Q39。随机森林的随机性在哪里?

在构建随机森林中的每棵树时，为了保证每棵树的独立性，我们通常采用两级随机：

1.  随机选取数据(采用套袋法)进行替换。
2.  随机选择m个特征。





Q41。说明深度学习模型和集成模型之间的相关性

深度学习模型如CNN和Transformer广泛采用集成策略。



Q42。分类器的集合可能不会比它的任何单个模型更精确。真或假?为什么?

真实的。通常，集成会改进模型，但这不是必要的。

