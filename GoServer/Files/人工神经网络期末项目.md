# 人工神经网络期末项目

**18340135 潘耀辉 计算机科学与技术（大数据）**

### 1.模型架构

参考`cnn`给出的`ThreeLayerConvNet`的结构：

1. 卷积
2. 激活
3. 池化
4. 全连接
5. 激活
6. 全连接

可以看到上面的结构中是没有BN层的，所以要提升模型性能，我们可以添加BN层。BN层的作用主要有三个： 加快网络的训练和收敛的速度 控制梯度爆炸防止梯度消失 防止过拟合。

#### **版本一：**

在卷积层和激活函数层之间加一层`BN`批量标准化层，并将一层卷积增加为两层卷积

1. 卷积
2. 批量标准化层
3. 激活
4. 池化
5. 卷积
6. 批量标准化层
7. 激活
8. 池化
9. 全连接
10. 激活
11. 全连接



##### 初始化：

```python
def __init__(
    self,
    input_dim=(3, 32, 32),
    num_filters=[16, 16],
    filter_size=7,
    hidden_dim=100,
    num_classes=10,
    weight_scale=1e-3,
    reg=0.0,
    dtype=np.float32,
):
```

```python
self.params['W1'] = weight_scale * np.random.randn(num_filters[0], C, filter_size, filter_size)
self.params['b1'] = np.zeros(num_filters[0])
self.params['gamma1'] = np.ones(num_filters[0])
self.params['beta1'] = np.zeros(num_filters[0])

self.params['W2'] = weight_scale * np.random.randn(num_filters[1], num_filters[0], filter_size, filter_size)
self.params['b2'] = np.zeros(num_filters[1])
self.params['gamma2'] = np.ones(num_filters[1])
self.params['beta2'] = np.zeros(num_filters[1])

self.params['W3'] = weight_scale * np.random.randn(int((H / 2)*(W / 2)*num_filters[1]*0.25), hidden_dim)
self.params['b3'] = np.zeros(hidden_dim)
self.params['W4'] = weight_scale * np.random.randn(hidden_dim, num_classes)
self.params['b4'] = np.zeros(num_classes)
```

**正向传播：**

```python
conv_forward_out_1, cache_forward_1 = conv_bn_relu_pool_forward(X, self.params['W1'], self.params['b1'],self.params['gamma1'], self.params['beta1'], conv_param, pool_param, bn_param)
conv_forward_out_2, cache_forward_2 = conv_bn_relu_pool_forward(conv_forward_out_1, self.params['W2'], self.params['b2'],self.params['gamma2'], self.params['beta2'], conv_param, pool_param, bn_param)
affine_forward_out_2, cache_forward_3 = affine_forward(conv_forward_out_2, self.params['W3'], self.params['b3'])
affine_relu_2, cache_relu_2 = relu_forward(affine_forward_out_2)
scores, cache_forward_4 = affine_forward(affine_relu_2, self.params['W4'], self.params['b4'])
```

**反向传播：**

```python
loss, dscore = softmax_loss(scores, y)
loss += self.reg * 0.5 * (np.sum(self.params['W1'] ** 2) + np.sum(self.params['W2'] ** 2) + np.sum(self.params['W3'] ** 2) + np.sum(self.params['W4'] ** 2))

dX4, grads['W4'], grads['b4'] = affine_backward(dscore, cache_forward_4)
dX3 = relu_backward(dX4, cache_relu_2)
dX3, grads['W3'], grads['b3'] = affine_backward(dX3, cache_forward_3)
dX2, grads['W2'], grads['b2'], grads['gamma2'], grads['beta2'] = conv_bn_relu_pool_backward(dX3, cache_forward_2)
dX1, grads['W1'], grads['b1'], grads['gamma1'], grads['beta1'] = conv_bn_relu_pool_backward(dX2, cache_forward_1)

grads['W4'] = grads['W4'] + self.reg * self.params['W4']
grads['W3'] = grads['W3'] + self.reg * self.params['W3']
grads['W2'] = grads['W2'] + self.reg * self.params['W2']
grads['W1'] = grads['W1'] + self.reg * self.params['W1']
```



#### **版本二：**

在卷积层和激活函数层之间加一层`BN`批量标准化层：

1. 卷积
2. 批量标准化层
3. 激活
4. 池化
5. 全连接
6. 激活
7. 全连接

##### **初始化：**

```python
def __init__(
    self,
    input_dim=(3, 32, 32),
    num_filters=32,
    filter_size=7,
    hidden_dim=100,
    num_classes=10,
    weight_scale=1e-3,
    reg=0.0,
    dtype=np.float32,
):
```

```python
self.params['W1'] = weight_scale * np.random.randn(num_filters, C, filter_size, filter_size)
self.params['b1'] = np.zeros(num_filters)
self.params['W2'] = weight_scale * np.random.randn(int((H / 2)*(W / 2))*num_filters, hidden_dim)
self.params['b2'] = np.zeros(hidden_dim)
self.params['W3'] = weight_scale * np.random.randn(hidden_dim, num_classes)
self.params['b3'] = np.zeros(num_classes)
self.params['gamma'] = np.ones(num_filters)
self.params['beta'] = np.zeros(num_filters)
```

##### 正向传播

```python
conv_forward_out_1, cache_forward_1 = conv_bn_relu_pool_forward(X, self.params['W1'], self.params['b1'],self.params['gamma'], self.params['beta'], conv_param, pool_param, bn_param)
affine_forward_out_2, cache_forward_2 = affine_forward(conv_forward_out_1, self.params['W2'], self.params['b2'])
affine_relu_2, cache_relu_2 = relu_forward(affine_forward_out_2)
scores, cache_forward_3 = affine_forward(affine_relu_2, self.params['W3'], self.params['b3'])
```

**反向传播**

```python
loss, dscore = softmax_loss(scores, y)
loss += self.reg * 0.5 * (np.sum(self.params['W1'] ** 2) + np.sum(self.params['W2'] ** 2) + np.sum(self.params['W3'] ** 2))

dX3, grads['W3'], grads['b3'] = affine_backward(dscore, cache_forward_3)
dX2 = relu_backward(dX3, cache_relu_2)
dX2, grads['W2'], grads['b2'] = affine_backward(dX2, cache_forward_2)
dX1, grads['W1'], grads['b1'], grads['gamma'], grads['beta'] = conv_bn_relu_pool_backward(dX2, cache_forward_1)

grads['W3'] = grads['W3'] + self.reg * self.params['W3']
grads['W2'] = grads['W2'] + self.reg * self.params['W2']
grads['W1'] = grads['W1'] + self.reg * self.params['W1']
```





### 2.调参过程

我的第一次尝试做出了两层卷积的版本一，我将该模型的过滤层设置2*16,其余结构与给出的`ThreeLayerConvNet`保持一致：

```python
model1 = MyConvNet2(weight_scale=0.001, hidden_dim=500, reg=0.001)
solver1 = Solver(model1, data,
                num_epochs=1, batch_size=50,
                update_rule='adam',
                optim_config={
                  'learning_rate': 1e-3,
                },
                verbose=True, print_every=20)
solver1.train()
```

![image-20210628230629000](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628230629000.png)

结果显示未达到60%的正确率，于是我猜想是不是可以加大迭代次数，从一次增加到五次：

![image-20210628230459052](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628230459052.png)

![image-20210628230513610](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628230513610.png)

结果很悲剧，效果在第一次迭代之后反而更差了，这显然是模型结构的问题，果断放弃该版本！



**第二个版本**，同样的采用相同的参数，直接将迭代次数增加到5：

```python
model_2 = MyConvNet2(weight_scale=0.001, hidden_dim=500, reg=0.001)

solver_2 = Solver(model_2, data,
                num_epochs=5, batch_size=50,
                update_rule='adam',
                optim_config={
                  'learning_rate': 1e-3,
                },
                verbose=True, print_every=20)
solver_2.train()
```

![image-20210628230822170](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628230822170.png)

![image-20210628230844489](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628230844489.png)



### 3.结果分析

**loss随迭代变化可视化如下：**

`ThreeLayerConvNet`

![image-20210628235416921](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628235416921.png)

版本一：

![image-20210628231108744](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628231108744.png)

版本二：

![image-20210628231141753](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628231141753.png)

由上图可见，采用了bn层之后，两个版本的loss收敛速度很接近，均显著强于`ThreeLayerConvNet`。



**预测准确率随迭代变化可视化如下：**

`ThreeLayerConvNet`

![image-20210628235516106](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628235516106.png)

版本一：

![image-20210628231510030](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628231510030.png)

版本二：

![image-20210628231539918](C:\Users\duya\AppData\Roaming\Typora\typora-user-images\image-20210628231539918.png)

可以看到，`ThreeLayerConvNet`的准确率在多次迭代之后才很艰难的突破60%，并且就稳定在了其附近。而版本二的正确率在第一次迭代之后就到达了50%左右，经过两次迭代就可以达到要求的60%，甚至在5次迭代之后仍然由上升的趋势，可见Bn层对于模型的效果的提升作用有多么巨大！

